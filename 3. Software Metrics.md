Below are the answers to the specified questions regarding software metrics for the flight simulator software designed for commercial pilots. Each answer is tailored to the context of the flight simulator, drawing on the concepts from the provided lecture slides and ensuring clarity and depth.

---

### Question 3.1: Select three software metrics (e.g., Cyclomatic complexity, code coverage, function points) relevant to the flight simulator software. Justify their importance and explain how you would measure them in this context.

**Answer:**

The flight simulator software, being a safety-critical system for training commercial pilots, requires robust metrics to ensure quality, reliability, and maintainability. Three relevant software metrics are **Cyclomatic Complexity**, **Code Coverage**, and **Function Points**. Below, I justify their importance and explain how they would be measured in this context.

1. **Cyclomatic Complexity**
   - **Importance**: Cyclomatic complexity measures the number of linearly independent paths through a program’s source code, indicating its logical complexity. For the flight simulator, modules like the autopilot or weather simulation involve intricate decision logic (e.g., handling turbulence or emergency scenarios). High complexity increases the risk of errors, which is unacceptable in a system where inaccuracies could affect pilot training. This metric helps identify overly complex code that may need refactoring to ensure reliability and ease of testing.
   - **Measurement**: Use a static analysis tool like SonarQube or Understand to analyze the source code of a module (e.g., the autopilot system). The tool calculates complexity based on the control flow graph, where complexity \( M = E - N + 2P \) (\( E \) = edges, \( N \) = nodes, \( P \) = connected components). For example, analyze the autopilot module’s decision points (e.g., if-else statements for altitude control) to compute the number of paths. A threshold (e.g., \( M > 10 \)) might indicate a need for simplification.
   - **Context Example**: In the flight simulator, the navigation module’s complexity is critical because it handles real-time route calculations. Measuring Cyclomatic complexity ensures the module remains testable and maintainable.

2. **Code Coverage**
   - **Importance**: Code coverage measures the percentage of code executed during testing, reflecting test thoroughness. For the flight simulator, comprehensive testing is vital to ensure all scenarios (e.g., normal flight, engine failure) are correctly simulated. High code coverage reduces the risk of untested paths causing failures during pilot training, aligning with safety-critical requirements.
   - **Measurement**: Use a code coverage tool like JaCoCo (for Java) or gcov (for C/C++) integrated into the CI pipeline. Run unit, integration, and system tests covering various flight scenarios (e.g., takeoff, landing, emergencies). The tool tracks which lines, branches, and conditions are executed, reporting coverage as replay percentage (e.g., 90% line coverage). For the flight simulator, aim for high coverage (e.g., 95%) for critical modules like the physics engine to ensure reliability.
   - **Context Example**: Test the weather simulation module with diverse inputs (e.g., calm, stormy, foggy conditions) to achieve high branch coverage, ensuring all weather-related code paths are validated.

3. **Function Points**
   - **Importance**: Function points measure the functionality delivered by the software, based on user interactions and data processing, making it ideal for estimating development effort and sizing the flight simulator project. Given the system’s complexity (e.g., cockpit controls, flight dynamics, logging), function points provide a standardized way to estimate effort, cost, and resource needs, independent of programming language or implementation details.
   - **Measurement**: Apply the IFPUG (International Function Point Users Group) method. Count five components: external inputs (e.g., pilot inputs via cockpit controls), external outputs (e.g., simulation visuals), external inquiries (e.g., querying flight parameters), internal logical files (e.g., aircraft model data), and external interface files (e.g., hardware interfaces). Assign complexity weights (low, average, high) and calculate total function points. For example, the cockpit interface might have 50 inputs, 30 outputs, and 20 inquiries, yielding a function point count used in effort estimation models like COCOMO.
   - **Context Example**: Function points help estimate the effort for adding a new aircraft model, ensuring accurate resource allocation for coding, testing, and validation.

**Summary**: Cyclomatic complexity ensures manageable and testable code, code coverage guarantees thorough testing, and function points support accurate project sizing. These metrics collectively address the flight simulator’s need for reliability, safety, and efficient development.

---

### Question 3.2: Evaluate the Cyclomatic complexity of a flight simulator module, such as the autopilot system. Draw a flow graph for a simplified version of this module and calculate its complexity. Suggest refactoring if needed.

**Answer:**

Let’s evaluate the Cyclomatic complexity of a simplified autopilot module in the flight simulator, responsible for maintaining a target altitude. The module checks the current altitude, compares it to the target, and adjusts the aircraft’s pitch accordingly, handling exceptions like turbulence.

**Simplified Autopilot Module Logic** (Pseudocode):
```python
def adjust_altitude(current_altitude, target_altitude, turbulence_factor):
    if current_altitude < target_altitude:
        if turbulence_factor > 0.5:
            increase_pitch(0.5)  # Gentle adjustment due to turbulence
        else:
            increase_pitch(1.0)  # Normal adjustment
    elif current_altitude > target_altitude:
        if turbulence_factor > 0.5:
            decrease_pitch(0.5)
        else:
            decrease_pitch(1.0)
    else:
        maintain_pitch()
    return new_pitch
```

**Flow Graph**:
The flow graph represents the control flow of the function. Nodes represent program statements, and edges represent control flow transitions.

- **Node 1**: Start (function entry)
- **Node 2**: Check `current_altitude < target_altitude`
- **Node 3**: Check `turbulence_factor > 0.5` (if true)
- **Node 4**: `increase_pitch(0.5)`
- **Node 5**: `increase_pitch(1.0)` (if false)
- **Node 6**: Check `current_altitude > target_altitude`
- **Node 7**: Check `turbulence_factor > 0.5` (if true)
- **Node 8**: `decrease_pitch(0.5)`
- **Node 9**: `decrease_pitch(1.0)` (if false)
- **Node 10**: `maintain_pitch()` (if altitude equals target)
- **Node 11**: Return `new_pitch` (exit)

**Edges**:
- 1 → 2
- 2 → 3 (true), 2 → 6 (false)
- 3 → 4 (true), 3 → 5 (false)
- 4 → 11, 5 → 11
- 6 → 7 (true), 6 → 10 (false)
- 7 → 8 (true), 7 → 9 (false)
- 8 → 11, 9 → 11
- 10 → 11

**Flow Graph Diagram** (Text-based):
```
       Start (1)
         |
         v
   +---(2) current_altitude < target_altitude? ---+
   |                                             |
   v                                             v
(3) turbulence_factor > 0.5?              (6) current_altitude > target_altitude?
   |         |                                   |         |
   v         v                                   v         v
(4) inc(0.5)(5) inc(1.0)                  (7) turbulence_factor > 0.5?  (10) maintain
   |         |                                   |         |
   v         v                                   v         v
   +--------(11) Return <-----------------(8) dec(0.5)(9) dec(1.0)
```

**Cyclomatic Complexity Calculation**:
Using the formula \( M = E - N + 2P \), where:
- \( E \) = number of edges = 12 (listed above)
- \( N \) = number of nodes = 11
- \( P \) = number of connected components = 1 (single graph)

\[
M = 12 - 11 + 2 \cdot 1 = 3
\]

Alternatively, count decision points (predicates):
- `current_altitude < target_altitude` (1 decision)
- `turbulence_factor > 0.5` (1 decision in true branch)
- `current_altitude > target_altitude` (1 decision, implicit else)
- `turbulence_factor > 0.5` (1 decision in true branch of second condition)

Total predicates = 3 (matches \( M = 3 \)).

**Interpretation**:
A Cyclomatic complexity of 3 is low, indicating the module is simple and testable. It suggests three independent paths through the code, which aligns with the three outcomes: increase pitch, decrease pitch, or maintain pitch. For a safety-critical system like the flight simulator, this low complexity is desirable, as it reduces the risk of errors and simplifies testing.

**Refactoring**:
No refactoring is needed, as the complexity is low (\( M = 3 \)) and below typical thresholds (e.g., \( M > 10 \)). The code is clear, with distinct paths for each case. However, for future enhancements (e.g., adding wind shear handling), monitor complexity to avoid exceeding \( M = 10 \). If complexity increases, consider:
- **Extracting sub-functions**: Move pitch adjustment logic to separate functions (e.g., `handle_increase_pitch(turbulence_factor)`).
- **Simplifying conditions**: Combine turbulence checks if possible, though this must maintain clarity for safety audits.

**Conclusion**: The autopilot module’s Cyclomatic complexity is 3, indicating simplicity and testability. No refactoring is required, but vigilance is needed for future feature additions.

---

### Question 3.3: Compare lines of code (LOC) and function points as metrics for estimating development effort for the flight simulator. Which would you use, and why, given its complexity and safety requirements?

**Answer:**

**Comparison of Lines of Code (LOC) and Function Points**:

1. **Lines of Code (LOC)**:
   - **Definition**: Counts the total number of source code lines, excluding comments and blank lines, to estimate software size.
   - **Advantages**:
     - Simple to measure using tools like CLOC or SonarQube.
     - Directly reflects coding effort, useful for low-level task estimation (e.g., coding a module).
     - Widely used in models like COCOMO for effort estimation.
   - **Disadvantages**:
     - Varies by programming language (e.g., C++ vs. Python), making cross-project comparisons difficult.
     - Ignores functionality; a concise but complex module may have fewer LOC than a verbose simple one.
     - Encourages bloated code, as developers might write more lines unnecessarily.
     - Less effective for non-coding tasks (e.g., configuring hardware interfaces).
   - **Measurement in Flight Simulator Context**: Count LOC for each module (e.g., simulation engine, cockpit UI) using a tool. Estimate total LOC (e.g., 40,000) and input into COCOMO for effort estimation. However, LOC may misrepresent effort for complex modules like flight dynamics, which require precise algorithms but fewer lines.

2. **Function Points**:
   - **Definition**: Measures software size based on functionality delivered to users, counting external inputs, outputs, inquiries, internal files, and external interfaces.
   - **Advantages**:
     - Language-agnostic, enabling consistent sizing across projects.
     - Focuses on user-visible functionality, aligning with the flight simulator’s goal of delivering realistic training scenarios.
     - Better for estimating effort for non-coding tasks (e.g., integrating with cockpit hardware).
     - Supports early estimation during requirements analysis, before coding begins.
   - **Disadvantages**:
     - More complex to calculate, requiring detailed analysis of functionality.
     - Subjective, as complexity weights (low, average, high) depend on analyst judgment.
     - Less intuitive for developers focused on code-level tasks.
   - **Measurement in Flight Simulator Context**: Use the IFPUG method to count function points. For example, identify 100 external inputs (pilot controls), 50 outputs (visual displays), 30 inquiries (flight data queries), 20 internal files (aircraft models), and 10 external interfaces (hardware). Assign weights and calculate total function points, then use in effort models like COCOMO or historical data.

**Comparison in Flight Simulator Context**:
- **Complexity**: The flight simulator is complex, with intricate modules (e.g., physics engine, real-time navigation) and diverse components (software, hardware integration). LOC may underestimate effort for algorithm-heavy modules, while function points capture the system’s functional scope, including non-coding tasks like hardware setup.
- **Safety Requirements**: The safety-critical nature demands thorough validation and testing. Function points better account for testing effort (e.g., validating pilot inputs/outputs), as they focus on functionality rather than code volume. LOC might overlook testing complexity for critical modules.
- **Estimation Needs**: Early estimation is crucial for budgeting and scheduling. Function points can be calculated during requirements analysis, providing a head start, whereas LOC requires coded modules, delaying estimates.

**Choice and Justification**:
I would use **Function Points** for estimating development effort for the flight simulator. Here’s why:
- **Functional Focus**: Function points align with the simulator’s goal of delivering realistic, reliable training scenarios, capturing the effort for user interactions (e.g., cockpit controls, visual feedback) and integrations (e.g., hardware).
- **Safety-Critical Suitability**: They account for validation and testing effort, critical for ensuring simulation accuracy, which LOC may undervalue.
- **Language Independence**: The simulator may use multiple languages (e.g., C++ for performance, Python for scripting). Function points provide a consistent metric, unlike LOC, which varies by language.
- **Early Estimation**: Function points enable effort estimation before coding, supporting project planning and stakeholder negotiations.
- **Holistic Scope**: They cover non-coding tasks (e.g., hardware integration, pilot feedback loops), which are significant in the simulator’s development.

While LOC could supplement function points for module-level estimates (e.g., coding the physics engine), function points offer a more comprehensive and reliable metric for the entire project, given its complexity and safety requirements.

---

### Question 3.4: Explain how software metrics can track the maintainability of the flight simulator over time, especially as new features (e.g., VR support) are added.

**Answer:**

Software metrics are critical for tracking the maintainability of the flight simulator software over time, ensuring it remains reliable, adaptable, and efficient as new features like VR support are added. Maintainability refers to the ease of modifying, fixing, and extending the software, which is vital for a safety-critical system used by commercial pilots. Below, I explain how specific metrics monitor maintainability and their application in the context of adding features.

**Key Metrics for Maintainability**:

1. **Cyclomatic Complexity**:
   - **Role in Maintainability**: Measures the logical complexity of code, indicating how difficult it is to understand, test, and modify. High complexity increases maintenance effort and error risk, especially when adding features like VR, which may introduce new decision paths (e.g., handling VR headset inputs).
   - **Tracking Over Time**: Regularly measure Cyclomatic complexity using tools like SonarQube for critical modules (e.g., rendering engine, input handler). As VR support is added, monitor complexity increases. For example, if the rendering module’s complexity rises from \( M = 5 \) to \( M = 15 \), it signals potential maintenance challenges, prompting refactoring (e.g., modularizing VR rendering logic).
   - **Example**: Adding VR support might add branches for headset tracking and stereoscopic rendering. If complexity exceeds a threshold (e.g., \( M > 10 \)), split the module into smaller functions to maintain clarity and testability.

2. **Code Coverage**:
   - **Role in Maintainability**: Reflects the extent of code tested, ensuring modifications don’t introduce regressions. High coverage ensures new features (e.g., VR interactions) are thoroughly validated, reducing maintenance issues from untested code paths.
   - **Tracking Over Time**: Use tools like JaCoCo to measure line and branch coverage during CI runs. As VR support is integrated, ensure tests cover new code (e.g., VR input handling). If coverage drops (e.g., from 95% to 80%), enhance tests to maintain confidence in modifications. Low coverage in VR modules could lead to bugs during updates, complicating maintenance.
   - **Example**: VR support requires testing new scenarios (e.g., head movement, 3D visuals). Maintaining 90%+ coverage ensures these additions don’t degrade reliability, easing future updates.

3. **Technical Debt Metrics (e.g., Code Smells, Duplication)**:
   - **Role in Maintainability**: Code smells (e.g., long methods, duplicated code) indicate poor design practices that hinder maintenance. Tracking these metrics identifies areas needing refactoring to accommodate new features without degrading quality.
   - **Tracking Over Time**: Use static analysis tools to detect code smells and duplication. As VR support is added, check for issues like duplicated VR rendering code across modules. A rise in code smells (e.g., from 10 to 50 per module) suggests technical debt, increasing maintenance effort. Refactor by consolidating shared logic into reusable functions.
   - **Example**: VR support might introduce duplicated code for handling headset inputs in multiple modules. Reducing duplication via a shared VR utility module improves maintainability for future enhancements.

4. **Maintainability Index**:
   - **Role in Maintainability**: A composite metric combining Cyclomatic complexity, code size, and other factors to quantify overall maintainability. A lower index indicates harder-to-maintain code, guiding improvement efforts.
   - **Tracking Over Time**: Calculate the Maintainability Index using tools like Visual Studio or SonarQube. Monitor changes as VR features are added. For instance, if the index drops from 80 to 60, investigate complexity or code smells in VR-related modules and refactor to restore maintainability.
   - **Example**: Adding VR might increase code size and complexity, lowering the Maintainability Index. Refactoring by modularizing VR logic can recover the index, ensuring long-term ease of updates.

**Application to Flight Simulator with VR Support**:
- **Scenario**: The flight simulator team adds VR support to enhance immersion, introducing new modules for headset tracking, 3D rendering, and user interactions. This increases code complexity and testing needs.
- **Metric Monitoring**:
  - **Cyclomatic Complexity**: The VR rendering module’s complexity rises to \( M = 12 \). Refactor by splitting into sub-modules (e.g., headset rendering, scene rendering) to keep \( M < 10 \).
  - **Code Coverage**: VR tests initially cover 75% of new code. Add tests for edge cases (e.g., headset disconnection) to reach 95%, ensuring reliable updates.
  - **Technical Debt**: Detect duplicated VR input code in cockpit and navigation modules. Consolidate into a shared VR handler to reduce maintenance overhead.
  - **Maintainability Index**: The index drops to 65 after VR integration. Address high-complexity modules and improve coverage to restore it to 80.
- **Outcome**: Regular metric tracking ensures VR additions don’t degrade maintainability. Refactoring and enhanced testing keep the simulator adaptable for future features (e.g., AI-based training scenarios).

**Additional Considerations**:
- **Trend Analysis**: Plot metrics over time to identify degradation trends. For example, a steady complexity increase signals a need for proactive refactoring before maintenance becomes burdensome.
- **Safety-Critical Focus**: In a safety-critical system, prioritize metrics ensuring reliability (e.g., coverage, complexity) to prevent errors during updates, which could affect pilot training.
- **Automation**: Integrate metric tracking into the CI pipeline to alert developers when thresholds are breached (e.g., coverage < 90%), enabling timely interventions.

**Conclusion**: Software metrics like Cyclomatic complexity, code coverage, technical debt, and Maintainability Index track the flight simulator’s maintainability by identifying complexity, testing gaps, and design flaws. As features like VR support are added, these metrics guide refactoring and testing efforts, ensuring the system remains easy to modify, reliable, and safe for commercial pilot training.

---

These answers provide a thorough and context-specific response to each question, leveraging the lecture slides’ concepts and tailoring them to the flight simulator’s unique requirements. Let me know if you need further clarification or additional assistance!